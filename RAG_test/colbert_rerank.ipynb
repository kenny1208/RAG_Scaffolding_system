{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.language_models.base import BaseLanguageModel\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "load_dotenv()\n",
    "key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and chunk data ---\n",
    "pdf_files = glob.glob(\"data/*.pdf\")\n",
    "pages = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = NLTKTextSplitter(chunk_size=5000, chunk_overlap=1000)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# --- First-stage vector DB ---\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=key, model=\"models/embedding-001\")\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"chroma_db_\")\n",
    "db.persist()\n",
    "db_connection = Chroma(persist_directory=\"chroma_db_\", embedding_function=embedding_model)\n",
    "\n",
    "first_retriever = db_connection.as_retriever(search_kwargs={\"k\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ColBERT Reranker: Use bi-encoder for pairwise re-ranking ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "def colbert_rerank(query, candidate_docs):\n",
    "    query_inputs = tokenizer(query, return_tensors='pt', truncation=True)\n",
    "    query_embedding = model(**query_inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    scored = []\n",
    "    for doc in candidate_docs:\n",
    "        inputs = tokenizer(doc.page_content, return_tensors='pt', truncation=True)\n",
    "        doc_embedding = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        sim = cosine_similarity(query_embedding.detach().numpy(), doc_embedding.detach().numpy())[0][0]\n",
    "        scored.append((sim, doc))\n",
    "\n",
    "    top_ranked = sorted(scored, key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, doc in top_ranked[:10]]  # Return top-10 reranked\n",
    "\n",
    "# --- Prompt Chain ---\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"You are a helpful academic assistant.\n",
    "    Please answer the question using only the provided context. \n",
    "    Do not include any explanations or additional information beyond what is asked.\n",
    "    If the context does not contain enough information, say \"I don't know\" rather than making up an answer.\"\"\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer: \"\"\")\n",
    "])\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# --- RAG Chain with rerank ---\n",
    "def rag_two_stage(question):\n",
    "    candidates = first_retriever.invoke(question)\n",
    "    reranked = colbert_rerank(question, candidates)\n",
    "    context = format_docs(reranked)\n",
    "    chain = (\n",
    "        {\"context\": lambda _: context, \"question\": lambda _: question}\n",
    "        | chat_template\n",
    "        | ChatGoogleGenerativeAI(google_api_key=key, model=\"gemini-1.5-flash-latest\")\n",
    "        | output_parser\n",
    "    )\n",
    "    answer = chain.invoke(question)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": [doc.page_content for doc in reranked]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What is the role of aggregate functions in SQL?\", \"ground_truth\": \"They perform calculations on sets of values.\"},\n",
    "    {\"question\": \"Define relationship in the E-R model.\", \"ground_truth\": \"An association among several entities.\"},\n",
    "    {\"question\": \"What is the purpose of a canonical cover?\", \"ground_truth\": \"A minimal set of functional dependencies equivalent to the original.\"},\n",
    "    {\"question\": \"What is the main goal of a DBMS?\", \"ground_truth\": \"To provide efficient and convenient access to data.\"},\n",
    "    {\"question\": \"List three applications of DBMS.\", \"ground_truth\": \"Banking, Airlines, Manufacturing.\"},\n",
    "    {\"question\": \"How does UNION differ from INTERSECT in SQL?\", \"ground_truth\": \"UNION merges results, INTERSECT finds common rows.\"},\n",
    "    {\"question\": \"Define data independence.\", \"ground_truth\": \"Ability to modify schema at one level without affecting the next.\"},\n",
    "    {\"question\": \"What is a superkey?\", \"ground_truth\": \"A set of attributes that uniquely identify an entity.\"},\n",
    "    {\"question\": \"What is normalization in databases?\", \"ground_truth\": \"The process of structuring a relational database to reduce redundancy.\"},\n",
    "    {\"question\": \"What does the SELECT clause do in SQL?\", \"ground_truth\": \"Specifies the attributes to retrieve.\"},\n",
    "    {\"question\": \"What is a functional dependency?\", \"ground_truth\": \"A constraint between two sets of attributes.\"},\n",
    "    {\"question\": \"What is data redundancy in file systems?\", \"ground_truth\": \"Duplication of information across files.\"},\n",
    "    {\"question\": \"What is a candidate key?\", \"ground_truth\": \"A minimal superkey.\"},\n",
    "    {\"question\": \"What is a derived attribute in E-R model?\", \"ground_truth\": \"An attribute whose values can be derived from other attributes.\"},\n",
    "   \n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 56/56 [00:56<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9048, 'answer_relevancy': 0.8318, 'context_precision': 0.7621, 'context_recall': 0.9286}\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "results = [rag_two_stage(row[\"question\"]) for row in dataset]\n",
    "\n",
    "for col in [\"answer\", \"contexts\"]:\n",
    "    if col in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(col)\n",
    "\n",
    "dataset = dataset.add_column(\"answer\", [r[\"answer\"] for r in results])\n",
    "dataset = dataset.add_column(\"contexts\", [r[\"contexts\"] for r in results])\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "my_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=\"key\")\n",
    "wrapped_llm = LangchainLLMWrapper(my_llm)\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness,answer_relevancy,context_precision, context_recall],\n",
    ")\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
