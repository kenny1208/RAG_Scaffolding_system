{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2323/2411306912.py:31: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n",
      "/tmp/ipykernel_2323/2411306912.py:32: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db_connection = Chroma(persist_directory=\"chroma_db_\", embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import nltk\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown as md\n",
    "from dotenv import load_dotenv\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_core.language_models.base import BaseLanguageModel\n",
    "import os\n",
    "import glob\n",
    "load_dotenv()  \n",
    "key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=key, \n",
    "                                   model=\"gemini-1.5-flash-latest\")\n",
    "pdf_files = glob.glob(\"data/*.pdf\")\n",
    "pages = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages.extend(loader.load_and_split())\n",
    "text_splitter = NLTKTextSplitter(chunk_size=5000, chunk_overlap=1000)\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=key, model=\"models/embedding-001\")\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"chroma_db_\")\n",
    "db.persist()\n",
    "db_connection = Chroma(persist_directory=\"chroma_db_\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 10})\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"You are a helpful academic assistant.\n",
    "    Please answer the question using only the provided context. \n",
    "    Do not include any explanations or additional information beyond what is asked.\n",
    "    If the context does not contain enough information, say \"I don't know\" rather than making up an answer.\"\"\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer: \"\"\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "def base_rag(query):\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    chain = (\n",
    "        {\"context\": lambda _: context, \"question\": lambda _: query}\n",
    "        | chat_template\n",
    "        | chat_model\n",
    "        | output_parser\n",
    "    )\n",
    "    \n",
    "    answer = chain.invoke(query)\n",
    "\n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": [doc.page_content for doc in docs]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What is the role of aggregate functions in SQL?\", \"ground_truth\": \"They perform calculations on sets of values.\"},\n",
    "    {\"question\": \"Define relationship in the E-R model.\", \"ground_truth\": \"An association among several entities.\"},\n",
    "    {\"question\": \"What is the purpose of a canonical cover?\", \"ground_truth\": \"A minimal set of functional dependencies equivalent to the original.\"},\n",
    "    {\"question\": \"What is the main goal of a DBMS?\", \"ground_truth\": \"To provide efficient and convenient access to data.\"},\n",
    "    {\"question\": \"List three applications of DBMS.\", \"ground_truth\": \"Banking, Airlines, Manufacturing.\"},\n",
    "    {\"question\": \"How does UNION differ from INTERSECT in SQL?\", \"ground_truth\": \"UNION merges results, INTERSECT finds common rows.\"},\n",
    "    {\"question\": \"Define data independence.\", \"ground_truth\": \"Ability to modify schema at one level without affecting the next.\"},\n",
    "    {\"question\": \"What is a superkey?\", \"ground_truth\": \"A set of attributes that uniquely identify an entity.\"},\n",
    "    {\"question\": \"What is normalization in databases?\", \"ground_truth\": \"The process of structuring a relational database to reduce redundancy.\"},\n",
    "    {\"question\": \"What does the SELECT clause do in SQL?\", \"ground_truth\": \"Specifies the attributes to retrieve.\"},\n",
    "    {\"question\": \"What is a functional dependency?\", \"ground_truth\": \"A constraint between two sets of attributes.\"},\n",
    "    {\"question\": \"What is data redundancy in file systems?\", \"ground_truth\": \"Duplication of information across files.\"},\n",
    "    {\"question\": \"What is a candidate key?\", \"ground_truth\": \"A minimal superkey.\"},\n",
    "    {\"question\": \"What is a derived attribute in E-R model?\", \"ground_truth\": \"An attribute whose values can be derived from other attributes.\"},\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 56/56 [00:53<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.7857, 'answer_relevancy': 0.7003, 'context_precision': 0.9573, 'context_recall': 0.8571}\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(examples)\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "results = [base_rag(row[\"question\"]) for row in dataset]\n",
    "\n",
    "for col in [\"answer\", \"contexts\"]:\n",
    "    if col in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(col)\n",
    "\n",
    "dataset = dataset.add_column(\"answer\", [r[\"answer\"] for r in results])\n",
    "dataset = dataset.add_column(\"contexts\", [r[\"contexts\"] for r in results])\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "my_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=\"key\")\n",
    "wrapped_llm = LangchainLLMWrapper(my_llm)\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    ")\n",
    "print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
